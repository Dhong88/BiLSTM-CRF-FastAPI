{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FASTAPI 100621(Final success) .ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "nWtxcaOD90bt",
        "outputId": "e27bf53f-7453-4157-9bf9-21c9dd66449c"
      },
      "source": [
        "!pip install colabcode\n",
        "!pip install fastapi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting colabcode\n",
            "  Downloading https://files.pythonhosted.org/packages/4b/90/f635c37e8e87cb9df76873edd35068e7b10cd20ac0ba4d2392ae7f307fe9/colabcode-0.3.0-py3-none-any.whl\n",
            "Collecting uvicorn==0.13.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ef/67/546c35e9fffb585ea0608ba3bdcafe17ae402e304367203d0b08d6c23051/uvicorn-0.13.1-py3-none-any.whl (45kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 4.7MB/s \n",
            "\u001b[?25hCollecting nest-asyncio==1.4.3\n",
            "  Downloading https://files.pythonhosted.org/packages/5c/33/10805a3359f56ac4f3b520e64b9d5e6a288d87be95777b8023c64cba60f1/nest_asyncio-1.4.3-py3-none-any.whl\n",
            "Collecting pyngrok>=5.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6b/4e/a2fe095bbe17cf26424c4abcd22a0490e22d01cc628f25af5e220ddbf6f0/pyngrok-5.0.5.tar.gz (745kB)\n",
            "\u001b[K     |████████████████████████████████| 747kB 9.6MB/s \n",
            "\u001b[?25hCollecting jupyterlab==3.0.7\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/90/27/149c258b8e80552ba1ad35636eca308776a284cb151cb8fcfff70adfbd0a/jupyterlab-3.0.7-py3-none-any.whl (8.3MB)\n",
            "\u001b[K     |████████████████████████████████| 8.3MB 15.3MB/s \n",
            "\u001b[?25hCollecting h11>=0.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/60/0f/7a0eeea938eaf61074f29fed9717f2010e8d0e0905d36b38d3275a1e4622/h11-0.12.0-py3-none-any.whl (54kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 6.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from uvicorn==0.13.1->colabcode) (3.7.4.3)\n",
            "Requirement already satisfied: click==7.* in /usr/local/lib/python3.7/dist-packages (from uvicorn==0.13.1->colabcode) (7.1.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from pyngrok>=5.0.0->colabcode) (3.13)\n",
            "Collecting jupyter-server~=1.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/29/b7/7377d007118f7798b21362a6c0a0bf20c93cdc19345105276a862e1263d6/jupyter_server-1.9.0-py3-none-any.whl (389kB)\n",
            "\u001b[K     |████████████████████████████████| 399kB 34.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: jupyter-core in /usr/local/lib/python3.7/dist-packages (from jupyterlab==3.0.7->colabcode) (4.7.1)\n",
            "Collecting nbclassic~=0.2\n",
            "  Downloading https://files.pythonhosted.org/packages/11/68/217ab6d4e4676dcfa4e855bb435469164a361a58e1856872cb06277f14b5/nbclassic-0.3.1-py3-none-any.whl\n",
            "Collecting jupyterlab-server~=2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/01/c4/461a38d71c5c9c756d8adf2e3acd6fd133512fae2bc22779c09e5b287149/jupyterlab_server-2.6.0-py3-none-any.whl (55kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 8.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: jinja2>=2.10 in /usr/local/lib/python3.7/dist-packages (from jupyterlab==3.0.7->colabcode) (2.11.3)\n",
            "Collecting tornado>=6.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/a8/9c5902233fa3c2e6a889cbd164333ddda5009669f494e3fadbeee2c03af5/tornado-6.1-cp37-cp37m-manylinux2010_x86_64.whl (428kB)\n",
            "\u001b[K     |████████████████████████████████| 430kB 29.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from jupyterlab==3.0.7->colabcode) (20.9)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from jupyterlab==3.0.7->colabcode) (5.5.0)\n",
            "Collecting websocket-client\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ca/5f/3c211d168b2e9f9342cfb53bcfc26aab0eac63b998015e7af7bcae66119d/websocket_client-1.1.0-py2.py3-none-any.whl (68kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 8.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: nbformat in /usr/local/lib/python3.7/dist-packages (from jupyter-server~=1.2->jupyterlab==3.0.7->colabcode) (5.1.3)\n",
            "Collecting requests-unixsocket\n",
            "  Downloading https://files.pythonhosted.org/packages/d0/63/97662a6f7175c08381447a09f6bc35464075f0ea6549cf6daf2668b51f04/requests_unixsocket-0.2.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.7/dist-packages (from jupyter-server~=1.2->jupyterlab==3.0.7->colabcode) (22.1.0)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from jupyter-server~=1.2->jupyterlab==3.0.7->colabcode) (0.2.0)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.7/dist-packages (from jupyter-server~=1.2->jupyterlab==3.0.7->colabcode) (20.1.0)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from jupyter-server~=1.2->jupyterlab==3.0.7->colabcode) (1.5.0)\n",
            "Requirement already satisfied: traitlets>=4.2.1 in /usr/local/lib/python3.7/dist-packages (from jupyter-server~=1.2->jupyterlab==3.0.7->colabcode) (5.0.5)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.7/dist-packages (from jupyter-server~=1.2->jupyterlab==3.0.7->colabcode) (0.10.1)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from jupyter-server~=1.2->jupyterlab==3.0.7->colabcode) (5.6.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.7/dist-packages (from jupyter-server~=1.2->jupyterlab==3.0.7->colabcode) (0.11.0)\n",
            "Collecting anyio<4,>=3.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5a/8c/6712b0aebe9b250736ec5dde99883b143290b49ecc2310eb583577e316aa/anyio-3.2.1-py3-none-any.whl (75kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 9.5MB/s \n",
            "\u001b[?25hCollecting jupyter-client>=6.1.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/77/e8/c3cf72a32a697256608d5fa96360c431adec6e1c6709ba7f13f99ff5ee04/jupyter_client-6.1.12-py3-none-any.whl (112kB)\n",
            "\u001b[K     |████████████████████████████████| 122kB 40.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: notebook<7 in /usr/local/lib/python3.7/dist-packages (from nbclassic~=0.2->jupyterlab==3.0.7->colabcode) (5.3.1)\n",
            "Collecting jsonschema>=3.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c5/8f/51e89ce52a085483359217bc72cdbf6e75ee595d5b1d4b5ade40c7e018b8/jsonschema-3.2.0-py2.py3-none-any.whl (56kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 7.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from jupyterlab-server~=2.0->jupyterlab==3.0.7->colabcode) (2.23.0)\n",
            "Collecting json5\n",
            "  Downloading https://files.pythonhosted.org/packages/7e/8e/ebde0a31c71e7098b3014faf46c80bdbcadb3c23b0ac7c7646b2af7d302e/json5-0.9.6-py2.py3-none-any.whl\n",
            "Requirement already satisfied: babel in /usr/local/lib/python3.7/dist-packages (from jupyterlab-server~=2.0->jupyterlab==3.0.7->colabcode) (2.9.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2>=2.10->jupyterlab==3.0.7->colabcode) (2.0.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->jupyterlab==3.0.7->colabcode) (2.4.7)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython->jupyterlab==3.0.7->colabcode) (57.0.0)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.7/dist-packages (from ipython->jupyterlab==3.0.7->colabcode) (4.8.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->jupyterlab==3.0.7->colabcode) (0.7.5)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->jupyterlab==3.0.7->colabcode) (2.6.1)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython->jupyterlab==3.0.7->colabcode) (1.0.18)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython->jupyterlab==3.0.7->colabcode) (0.8.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->jupyterlab==3.0.7->colabcode) (4.4.2)\n",
            "Requirement already satisfied: urllib3>=1.8 in /usr/local/lib/python3.7/dist-packages (from requests-unixsocket->jupyter-server~=1.2->jupyterlab==3.0.7->colabcode) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from argon2-cffi->jupyter-server~=1.2->jupyterlab==3.0.7->colabcode) (1.15.0)\n",
            "Requirement already satisfied: cffi>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from argon2-cffi->jupyter-server~=1.2->jupyterlab==3.0.7->colabcode) (1.14.5)\n",
            "Requirement already satisfied: ptyprocess; os_name != \"nt\" in /usr/local/lib/python3.7/dist-packages (from terminado>=0.8.3->jupyter-server~=1.2->jupyterlab==3.0.7->colabcode) (0.7.0)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter-server~=1.2->jupyterlab==3.0.7->colabcode) (3.3.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter-server~=1.2->jupyterlab==3.0.7->colabcode) (0.7.1)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter-server~=1.2->jupyterlab==3.0.7->colabcode) (1.4.3)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter-server~=1.2->jupyterlab==3.0.7->colabcode) (0.3)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter-server~=1.2->jupyterlab==3.0.7->colabcode) (0.8.4)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter-server~=1.2->jupyterlab==3.0.7->colabcode) (0.5.0)\n",
            "Collecting sniffio>=1.1\n",
            "  Downloading https://files.pythonhosted.org/packages/52/b0/7b2e028b63d092804b6794595871f936aafa5e9322dcaaad50ebf67445b3/sniffio-1.2.0-py3-none-any.whl\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.7/dist-packages (from anyio<4,>=3.1.0->jupyter-server~=1.2->jupyterlab==3.0.7->colabcode) (2.10)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from jupyter-client>=6.1.1->jupyter-server~=1.2->jupyterlab==3.0.7->colabcode) (2.8.1)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.7/dist-packages (from notebook<7->nbclassic~=0.2->jupyterlab==3.0.7->colabcode) (4.10.1)\n",
            "Requirement already satisfied: pyrsistent>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=3.0.1->jupyterlab-server~=2.0->jupyterlab==3.0.7->colabcode) (0.17.3)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from jsonschema>=3.0.1->jupyterlab-server~=2.0->jupyterlab==3.0.7->colabcode) (4.5.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=3.0.1->jupyterlab-server~=2.0->jupyterlab==3.0.7->colabcode) (21.2.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->jupyterlab-server~=2.0->jupyterlab==3.0.7->colabcode) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->jupyterlab-server~=2.0->jupyterlab==3.0.7->colabcode) (2021.5.30)\n",
            "Requirement already satisfied: pytz>=2015.7 in /usr/local/lib/python3.7/dist-packages (from babel->jupyterlab-server~=2.0->jupyterlab==3.0.7->colabcode) (2018.9)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->jupyterlab==3.0.7->colabcode) (0.2.5)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.0.0->argon2-cffi->jupyter-server~=1.2->jupyterlab==3.0.7->colabcode) (2.20)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->jupyter-server~=1.2->jupyterlab==3.0.7->colabcode) (0.5.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->jsonschema>=3.0.1->jupyterlab-server~=2.0->jupyterlab==3.0.7->colabcode) (3.4.1)\n",
            "Building wheels for collected packages: pyngrok\n",
            "  Building wheel for pyngrok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyngrok: filename=pyngrok-5.0.5-cp37-none-any.whl size=19262 sha256=fbe594bc74424ebfdea1938ed01ba0c327627e935d9632c1530f25c3fbcb464a\n",
            "  Stored in directory: /root/.cache/pip/wheels/0c/13/64/5ebbcc22eaf53fdf5766b397c1fb17c83f5775fdccf0ea1b88\n",
            "Successfully built pyngrok\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement tornado~=5.1.0; python_version >= \"3.0\", but you'll have tornado 6.1 which is incompatible.\u001b[0m\n",
            "Installing collected packages: h11, uvicorn, nest-asyncio, pyngrok, websocket-client, requests-unixsocket, tornado, sniffio, anyio, jupyter-client, jupyter-server, nbclassic, jsonschema, json5, jupyterlab-server, jupyterlab, colabcode\n",
            "  Found existing installation: nest-asyncio 1.5.1\n",
            "    Uninstalling nest-asyncio-1.5.1:\n",
            "      Successfully uninstalled nest-asyncio-1.5.1\n",
            "  Found existing installation: tornado 5.1.1\n",
            "    Uninstalling tornado-5.1.1:\n",
            "      Successfully uninstalled tornado-5.1.1\n",
            "  Found existing installation: jupyter-client 5.3.5\n",
            "    Uninstalling jupyter-client-5.3.5:\n",
            "      Successfully uninstalled jupyter-client-5.3.5\n",
            "  Found existing installation: jsonschema 2.6.0\n",
            "    Uninstalling jsonschema-2.6.0:\n",
            "      Successfully uninstalled jsonschema-2.6.0\n",
            "Successfully installed anyio-3.2.1 colabcode-0.3.0 h11-0.12.0 json5-0.9.6 jsonschema-3.2.0 jupyter-client-6.1.12 jupyter-server-1.9.0 jupyterlab-3.0.7 jupyterlab-server-2.6.0 nbclassic-0.3.1 nest-asyncio-1.4.3 pyngrok-5.0.5 requests-unixsocket-0.2.0 sniffio-1.2.0 tornado-6.1 uvicorn-0.13.1 websocket-client-1.1.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "jupyter_client",
                  "tornado"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Collecting fastapi\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/dc/a8/a6be420593c4061c086e6d2ba47db46401d9af2b98b6cd33d35284f706d3/fastapi-0.65.2-py3-none-any.whl (51kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 2.6MB/s \n",
            "\u001b[?25hCollecting starlette==0.14.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/15/34/db1890f442a1cd3a2c761f4109a0eb4e63503218d70a8c8e97faa09a5500/starlette-0.14.2-py3-none-any.whl (60kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 7.5MB/s \n",
            "\u001b[?25hCollecting pydantic!=1.7,!=1.7.1,!=1.7.2,!=1.7.3,!=1.8,!=1.8.1,<2.0.0,>=1.6.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9f/f2/2d5425efe57f6c4e06cbe5e587c1fd16929dcf0eb90bd4d3d1e1c97d1151/pydantic-1.8.2-cp37-cp37m-manylinux2014_x86_64.whl (10.1MB)\n",
            "\u001b[K     |████████████████████████████████| 10.1MB 205kB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from pydantic!=1.7,!=1.7.1,!=1.7.2,!=1.7.3,!=1.8,!=1.8.1,<2.0.0,>=1.6.2->fastapi) (3.7.4.3)\n",
            "Installing collected packages: starlette, pydantic, fastapi\n",
            "Successfully installed fastapi-0.65.2 pydantic-1.8.2 starlette-0.14.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3OkkNkPLSHyf",
        "outputId": "f9f20834-45ed-4b5d-d3f9-c05f83169991"
      },
      "source": [
        "!pip install git+https://www.github.com/keras-team/keras-contrib.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://www.github.com/keras-team/keras-contrib.git\n",
            "  Cloning https://www.github.com/keras-team/keras-contrib.git to /tmp/pip-req-build-3073c8bm\n",
            "  Running command git clone -q https://www.github.com/keras-team/keras-contrib.git /tmp/pip-req-build-3073c8bm\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (from keras-contrib==2.0.8) (2.4.3)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (3.1.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (1.4.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (3.13)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (1.19.5)\n",
            "Requirement already satisfied: cached-property; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from h5py->keras->keras-contrib==2.0.8) (1.5.2)\n",
            "Building wheels for collected packages: keras-contrib\n",
            "  Building wheel for keras-contrib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-contrib: filename=keras_contrib-2.0.8-cp37-none-any.whl size=101078 sha256=2cd901d7d59cac75cd78f3bfc494119a8b4308e10d2834d566a632379fcb5134\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-tmyci4zn/wheels/11/27/c8/4ed56de7b55f4f61244e2dc6ef3cdbaff2692527a2ce6502ba\n",
            "Successfully built keras-contrib\n",
            "Installing collected packages: keras-contrib\n",
            "Successfully installed keras-contrib-2.0.8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "czNVntA-SgUy",
        "outputId": "f7d26c27-2d3a-4112-ddc6-3d634ccffe26"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ovU8A_TrELGI",
        "outputId": "da08c160-343e-4ec4-cc3d-f2b0bc2333f2"
      },
      "source": [
        "!pip install git+https://www.github.com/keras-team/keras-contrib.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://www.github.com/keras-team/keras-contrib.git\n",
            "  Cloning https://www.github.com/keras-team/keras-contrib.git to /tmp/pip-req-build-crlcr5zs\n",
            "  Running command git clone -q https://www.github.com/keras-team/keras-contrib.git /tmp/pip-req-build-crlcr5zs\n",
            "Requirement already satisfied (use --upgrade to upgrade): keras-contrib==2.0.8 from git+https://www.github.com/keras-team/keras-contrib.git in /usr/local/lib/python3.7/dist-packages\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (from keras-contrib==2.0.8) (2.4.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (3.13)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (1.19.5)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (3.1.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (1.4.1)\n",
            "Requirement already satisfied: cached-property; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from h5py->keras->keras-contrib==2.0.8) (1.5.2)\n",
            "Building wheels for collected packages: keras-contrib\n",
            "  Building wheel for keras-contrib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-contrib: filename=keras_contrib-2.0.8-cp37-none-any.whl size=101078 sha256=059e23234e5a06d214bffc8e83a1704c38ff928063ac5c2bbef0c27a318b523e\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-bc8zebgp/wheels/11/27/c8/4ed56de7b55f4f61244e2dc6ef3cdbaff2692527a2ce6502ba\n",
            "Successfully built keras-contrib\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uybLas46VG-7"
      },
      "source": [
        "#Initialise model \n",
        "import keras\n",
        "import tensorflow as tf\n",
        "from keras.models import Model, load_model\n",
        "from keras.layers import Input, Embedding, LSTM, Dense, Dropout, TimeDistributed, Bidirectional\n",
        "from keras_contrib.layers import CRF\n",
        "from keras_contrib.losses import  crf_loss\n",
        "from keras_contrib.metrics import crf_viterbi_accuracy\n",
        "import pickle\n",
        "from fastapi import FastAPI\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nTHv9CfZD-K3"
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "\n",
        "import warnings\n",
        "\n",
        "from keras import backend as K\n",
        "from keras import activations\n",
        "from keras import initializers\n",
        "from keras import regularizers\n",
        "from keras import constraints\n",
        "from keras.layers import Layer\n",
        "from keras.layers import InputSpec\n",
        "\n",
        "from keras_contrib.losses import crf_loss\n",
        "from keras_contrib.metrics import crf_marginal_accuracy\n",
        "from keras_contrib.metrics import crf_viterbi_accuracy\n",
        "from keras_contrib.utils.test_utils import to_tuple\n",
        "\n",
        "\n",
        "class CRF(Layer):\n",
        "    \"\"\"An implementation of linear chain conditional random field (CRF).\n",
        "    An linear chain CRF is defined to maximize the following likelihood function:\n",
        "    $$ L(W, U, b; y_1, ..., y_n) := \\frac{1}{Z}\n",
        "    \\sum_{y_1, ..., y_n} \\exp(-a_1' y_1 - a_n' y_n\n",
        "        - \\sum_{k=1^n}((f(x_k' W + b) y_k) + y_1' U y_2)), $$\n",
        "    where:\n",
        "        $Z$: normalization constant\n",
        "        $x_k, y_k$:  inputs and outputs\n",
        "    This implementation has two modes for optimization:\n",
        "    1. (`join mode`) optimized by maximizing join likelihood,\n",
        "    which is optimal in theory of statistics.\n",
        "       Note that in this case, CRF must be the output/last layer.\n",
        "    2. (`marginal mode`) return marginal probabilities on each time\n",
        "    step and optimized via composition\n",
        "       likelihood (product of marginal likelihood), i.e.,\n",
        "       using `categorical_crossentropy` loss.\n",
        "       Note that in this case, CRF can be either the last layer or an\n",
        "       intermediate layer (though not explored).\n",
        "    For prediction (test phrase), one can choose either Viterbi\n",
        "    best path (class indices) or marginal\n",
        "    probabilities if probabilities are needed.\n",
        "    However, if one chooses *join mode* for training,\n",
        "    Viterbi output is typically better than marginal output,\n",
        "    but the marginal output will still perform\n",
        "    reasonably close, while if *marginal mode* is used for training,\n",
        "    marginal output usually performs\n",
        "    much better. The default behavior and `metrics.crf_accuracy`\n",
        "    is set according to this observation.\n",
        "    In addition, this implementation supports masking and accepts either\n",
        "    onehot or sparse target.\n",
        "    If you open a issue or a pull request about CRF, please\n",
        "    add 'cc @lzfelix' to notify Luiz Felix.\n",
        "    # Examples\n",
        "    ```python\n",
        "        from keras_contrib.layers import CRF\n",
        "        from keras_contrib.losses import crf_loss\n",
        "        from keras_contrib.metrics import crf_viterbi_accuracy\n",
        "        model = Sequential()\n",
        "        model.add(Embedding(3001, 300, mask_zero=True)(X)\n",
        "        # use learn_mode = 'join', test_mode = 'viterbi',\n",
        "        # sparse_target = True (label indice output)\n",
        "        crf = CRF(10, sparse_target=True)\n",
        "        model.add(crf)\n",
        "        # crf_accuracy is default to Viterbi acc if using join-mode (default).\n",
        "        # One can add crf.marginal_acc if interested, but may slow down learning\n",
        "        model.compile('adam', loss=crf_loss, metrics=[crf_viterbi_accuracy])\n",
        "        # y must be label indices (with shape 1 at dim 3) here,\n",
        "        # since `sparse_target=True`\n",
        "        model.fit(x, y)\n",
        "        # prediction give onehot representation of Viterbi best path\n",
        "        y_hat = model.predict(x_test)\n",
        "    ```\n",
        "    The following snippet shows how to load a persisted\n",
        "    model that uses the CRF layer:\n",
        "    ```python\n",
        "        from keras.models import load_model\n",
        "        from keras_contrib.losses import import crf_loss\n",
        "        from keras_contrib.metrics import crf_viterbi_accuracy\n",
        "        custom_objects={'CRF': CRF,\n",
        "                        'crf_loss': crf_loss,\n",
        "                        'crf_viterbi_accuracy': crf_viterbi_accuracy}\n",
        "        loaded_model = load_model('<path_to_model>',\n",
        "                                  custom_objects=custom_objects)\n",
        "    ```\n",
        "    # Arguments\n",
        "        units: Positive integer, dimensionality of the output space.\n",
        "        learn_mode: Either 'join' or 'marginal'.\n",
        "            The former train the model by maximizing join likelihood while the latter\n",
        "            maximize the product of marginal likelihood over all time steps.\n",
        "            One should use `losses.crf_nll` for 'join' mode\n",
        "            and `losses.categorical_crossentropy` or\n",
        "            `losses.sparse_categorical_crossentropy` for\n",
        "            `marginal` mode.  For convenience, simply\n",
        "            use `losses.crf_loss`, which will decide the proper loss as described.\n",
        "        test_mode: Either 'viterbi' or 'marginal'.\n",
        "            The former is recommended and as default when `learn_mode = 'join'` and\n",
        "            gives one-hot representation of the best path at test (prediction) time,\n",
        "            while the latter is recommended and chosen as default\n",
        "            when `learn_mode = 'marginal'`,\n",
        "            which produces marginal probabilities for each time step.\n",
        "            For evaluating metrics, one should\n",
        "            use `metrics.crf_viterbi_accuracy` for 'viterbi' mode and\n",
        "            'metrics.crf_marginal_accuracy' for 'marginal' mode, or\n",
        "            simply use `metrics.crf_accuracy` for\n",
        "            both which automatically decides it as described.\n",
        "            One can also use both for evaluation at training.\n",
        "        sparse_target: Boolean (default False) indicating\n",
        "            if provided labels are one-hot or\n",
        "            indices (with shape 1 at dim 3).\n",
        "        use_boundary: Boolean (default True) indicating if trainable\n",
        "            start-end chain energies\n",
        "            should be added to model.\n",
        "        use_bias: Boolean, whether the layer uses a bias vector.\n",
        "        kernel_initializer: Initializer for the `kernel` weights matrix,\n",
        "            used for the linear transformation of the inputs.\n",
        "            (see [initializers](../initializers.md)).\n",
        "        chain_initializer: Initializer for the `chain_kernel` weights matrix,\n",
        "            used for the CRF chain energy.\n",
        "            (see [initializers](../initializers.md)).\n",
        "        boundary_initializer: Initializer for the `left_boundary`,\n",
        "            'right_boundary' weights vectors,\n",
        "            used for the start/left and end/right boundary energy.\n",
        "            (see [initializers](../initializers.md)).\n",
        "        bias_initializer: Initializer for the bias vector\n",
        "            (see [initializers](../initializers.md)).\n",
        "        activation: Activation function to use\n",
        "            (see [activations](../activations.md)).\n",
        "            If you pass None, no activation is applied\n",
        "            (ie. \"linear\" activation: `a(x) = x`).\n",
        "        kernel_regularizer: Regularizer function applied to\n",
        "            the `kernel` weights matrix\n",
        "            (see [regularizer](../regularizers.md)).\n",
        "        chain_regularizer: Regularizer function applied to\n",
        "            the `chain_kernel` weights matrix\n",
        "            (see [regularizer](../regularizers.md)).\n",
        "        boundary_regularizer: Regularizer function applied to\n",
        "            the 'left_boundary', 'right_boundary' weight vectors\n",
        "            (see [regularizer](../regularizers.md)).\n",
        "        bias_regularizer: Regularizer function applied to the bias vector\n",
        "            (see [regularizer](../regularizers.md)).\n",
        "        kernel_constraint: Constraint function applied to\n",
        "            the `kernel` weights matrix\n",
        "            (see [constraints](../constraints.md)).\n",
        "        chain_constraint: Constraint function applied to\n",
        "            the `chain_kernel` weights matrix\n",
        "            (see [constraints](../constraints.md)).\n",
        "        boundary_constraint: Constraint function applied to\n",
        "            the `left_boundary`, `right_boundary` weights vectors\n",
        "            (see [constraints](../constraints.md)).\n",
        "        bias_constraint: Constraint function applied to the bias vector\n",
        "            (see [constraints](../constraints.md)).\n",
        "        input_dim: dimensionality of the input (integer).\n",
        "            This argument (or alternatively, the keyword argument `input_shape`)\n",
        "            is required when using this layer as the first layer in a model.\n",
        "        unroll: Boolean (default False). If True, the network will be\n",
        "            unrolled, else a symbolic loop will be used.\n",
        "            Unrolling can speed-up a RNN, although it tends\n",
        "            to be more memory-intensive.\n",
        "            Unrolling is only suitable for short sequences.\n",
        "    # Input shape\n",
        "        3D tensor with shape `(nb_samples, timesteps, input_dim)`.\n",
        "    # Output shape\n",
        "        3D tensor with shape `(nb_samples, timesteps, units)`.\n",
        "    # Masking\n",
        "        This layer supports masking for input data with a variable number\n",
        "        of timesteps. To introduce masks to your data,\n",
        "        use an [Embedding](embeddings.md) layer with the `mask_zero` parameter\n",
        "        set to `True`.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, units,\n",
        "                 learn_mode='join',\n",
        "                 test_mode=None,\n",
        "                 sparse_target=False,\n",
        "                 use_boundary=True,\n",
        "                 use_bias=True,\n",
        "                 activation='linear',\n",
        "                 kernel_initializer='glorot_uniform',\n",
        "                 chain_initializer='orthogonal',\n",
        "                 bias_initializer='zeros',\n",
        "                 boundary_initializer='zeros',\n",
        "                 kernel_regularizer=None,\n",
        "                 chain_regularizer=None,\n",
        "                 boundary_regularizer=None,\n",
        "                 bias_regularizer=None,\n",
        "                 kernel_constraint=None,\n",
        "                 chain_constraint=None,\n",
        "                 boundary_constraint=None,\n",
        "                 bias_constraint=None,\n",
        "                 input_dim=None,\n",
        "                 unroll=False,\n",
        "                 **kwargs):\n",
        "        super(CRF, self).__init__(**kwargs)\n",
        "        self.supports_masking = True\n",
        "        self.units = units\n",
        "        self.learn_mode = learn_mode\n",
        "        assert self.learn_mode in ['join', 'marginal']\n",
        "        self.test_mode = test_mode\n",
        "        if self.test_mode is None:\n",
        "            self.test_mode = 'viterbi' if self.learn_mode == 'join' else 'marginal'\n",
        "        else:\n",
        "            assert self.test_mode in ['viterbi', 'marginal']\n",
        "        self.sparse_target = sparse_target\n",
        "        self.use_boundary = use_boundary\n",
        "        self.use_bias = use_bias\n",
        "\n",
        "        self.activation = activations.get(activation)\n",
        "\n",
        "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
        "        self.chain_initializer = initializers.get(chain_initializer)\n",
        "        self.boundary_initializer = initializers.get(boundary_initializer)\n",
        "        self.bias_initializer = initializers.get(bias_initializer)\n",
        "\n",
        "        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n",
        "        self.chain_regularizer = regularizers.get(chain_regularizer)\n",
        "        self.boundary_regularizer = regularizers.get(boundary_regularizer)\n",
        "        self.bias_regularizer = regularizers.get(bias_regularizer)\n",
        "\n",
        "        self.kernel_constraint = constraints.get(kernel_constraint)\n",
        "        self.chain_constraint = constraints.get(chain_constraint)\n",
        "        self.boundary_constraint = constraints.get(boundary_constraint)\n",
        "        self.bias_constraint = constraints.get(bias_constraint)\n",
        "\n",
        "        self.unroll = unroll\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        input_shape = to_tuple(input_shape)\n",
        "        self.input_spec = [InputSpec(shape=input_shape)]\n",
        "        self.input_dim = input_shape[-1]\n",
        "\n",
        "        self.kernel = self.add_weight(shape=(self.input_dim, self.units),\n",
        "                                      name='kernel',\n",
        "                                      initializer=self.kernel_initializer,\n",
        "                                      regularizer=self.kernel_regularizer,\n",
        "                                      constraint=self.kernel_constraint)\n",
        "        self.chain_kernel = self.add_weight(shape=(self.units, self.units),\n",
        "                                            name='chain_kernel',\n",
        "                                            initializer=self.chain_initializer,\n",
        "                                            regularizer=self.chain_regularizer,\n",
        "                                            constraint=self.chain_constraint)\n",
        "        if self.use_bias:\n",
        "            self.bias = self.add_weight(shape=(self.units,),\n",
        "                                        name='bias',\n",
        "                                        initializer=self.bias_initializer,\n",
        "                                        regularizer=self.bias_regularizer,\n",
        "                                        constraint=self.bias_constraint)\n",
        "        else:\n",
        "            self.bias = 0\n",
        "\n",
        "        if self.use_boundary:\n",
        "            self.left_boundary = self.add_weight(shape=(self.units,),\n",
        "                                                 name='left_boundary',\n",
        "                                                 initializer=self.boundary_initializer,\n",
        "                                                 regularizer=self.boundary_regularizer,\n",
        "                                                 constraint=self.boundary_constraint)\n",
        "            self.right_boundary = self.add_weight(shape=(self.units,),\n",
        "                                                  name='right_boundary',\n",
        "                                                  initializer=self.boundary_initializer,\n",
        "                                                  regularizer=self.boundary_regularizer,\n",
        "                                                  constraint=self.boundary_constraint)\n",
        "        self.built = True\n",
        "\n",
        "    def call(self, X, mask=None):\n",
        "        if mask is not None:\n",
        "            assert K.ndim(mask) == 2, 'Input mask to CRF must have dim 2 if not None'\n",
        "\n",
        "        if self.test_mode == 'viterbi':\n",
        "            test_output = self.viterbi_decoding(X, mask)\n",
        "        else:\n",
        "            test_output = self.get_marginal_prob(X, mask)\n",
        "\n",
        "        self.uses_learning_phase = True\n",
        "        if self.learn_mode == 'join':\n",
        "            train_output = K.zeros_like(K.dot(X, self.kernel))\n",
        "            out = K.in_train_phase(train_output, test_output)\n",
        "        else:\n",
        "            if self.test_mode == 'viterbi':\n",
        "                train_output = self.get_marginal_prob(X, mask)\n",
        "                out = K.in_train_phase(train_output, test_output)\n",
        "            else:\n",
        "                out = test_output\n",
        "        return out\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape[:2] + (self.units,)\n",
        "\n",
        "    def compute_mask(self, input, mask=None):\n",
        "        if mask is not None and self.learn_mode == 'join':\n",
        "            return K.any(mask, axis=1)\n",
        "        return mask\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {\n",
        "            'units': self.units,\n",
        "            'learn_mode': self.learn_mode,\n",
        "            'test_mode': self.test_mode,\n",
        "            'use_boundary': self.use_boundary,\n",
        "            'use_bias': self.use_bias,\n",
        "            'sparse_target': self.sparse_target,\n",
        "            'kernel_initializer': initializers.serialize(self.kernel_initializer),\n",
        "            'chain_initializer': initializers.serialize(self.chain_initializer),\n",
        "            'boundary_initializer': initializers.serialize(\n",
        "                self.boundary_initializer),\n",
        "            'bias_initializer': initializers.serialize(self.bias_initializer),\n",
        "            'activation': activations.serialize(self.activation),\n",
        "            'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n",
        "            'chain_regularizer': regularizers.serialize(self.chain_regularizer),\n",
        "            'boundary_regularizer': regularizers.serialize(\n",
        "                self.boundary_regularizer),\n",
        "            'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n",
        "            'kernel_constraint': constraints.serialize(self.kernel_constraint),\n",
        "            'chain_constraint': constraints.serialize(self.chain_constraint),\n",
        "            'boundary_constraint': constraints.serialize(self.boundary_constraint),\n",
        "            'bias_constraint': constraints.serialize(self.bias_constraint),\n",
        "            'input_dim': self.input_dim,\n",
        "            'unroll': self.unroll}\n",
        "        base_config = super(CRF, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))\n",
        "\n",
        "    @property\n",
        "    def loss_function(self):\n",
        "        warnings.warn('CRF.loss_function is deprecated '\n",
        "                      'and it might be removed in the future. Please '\n",
        "                      'use losses.crf_loss instead.')\n",
        "        return crf_loss\n",
        "\n",
        "    @property\n",
        "    def accuracy(self):\n",
        "        warnings.warn('CRF.accuracy is deprecated and it '\n",
        "                      'might be removed in the future. Please '\n",
        "                      'use metrics.crf_accuracy')\n",
        "        if self.test_mode == 'viterbi':\n",
        "            return crf_viterbi_accuracy\n",
        "        else:\n",
        "            return crf_marginal_accuracy\n",
        "\n",
        "    @property\n",
        "    def viterbi_acc(self):\n",
        "        warnings.warn('CRF.viterbi_acc is deprecated and it might '\n",
        "                      'be removed in the future. Please '\n",
        "                      'use metrics.viterbi_acc instead.')\n",
        "        return crf_viterbi_accuracy\n",
        "\n",
        "    @property\n",
        "    def marginal_acc(self):\n",
        "        warnings.warn('CRF.moarginal_acc is deprecated and it '\n",
        "                      'might be removed in the future. Please '\n",
        "                      'use metrics.marginal_acc instead.')\n",
        "        return crf_marginal_accuracy\n",
        "\n",
        "    @staticmethod\n",
        "    def softmaxNd(x, axis=-1):\n",
        "        m = K.max(x, axis=axis, keepdims=True)\n",
        "        exp_x = K.exp(x - m)\n",
        "        prob_x = exp_x / K.sum(exp_x, axis=axis, keepdims=True)\n",
        "        return prob_x\n",
        "\n",
        "    @staticmethod\n",
        "    def shift_left(x, offset=1):\n",
        "        assert offset > 0\n",
        "        return K.concatenate([x[:, offset:], K.zeros_like(x[:, :offset])], axis=1)\n",
        "\n",
        "    @staticmethod\n",
        "    def shift_right(x, offset=1):\n",
        "        assert offset > 0\n",
        "        return K.concatenate([K.zeros_like(x[:, :offset]), x[:, :-offset]], axis=1)\n",
        "\n",
        "    def add_boundary_energy(self, energy, mask, start, end):\n",
        "        start = K.expand_dims(K.expand_dims(start, 0), 0)\n",
        "        end = K.expand_dims(K.expand_dims(end, 0), 0)\n",
        "        if mask is None:\n",
        "            energy = K.concatenate([energy[:, :1, :] + start, energy[:, 1:, :]],\n",
        "                                   axis=1)\n",
        "            energy = K.concatenate([energy[:, :-1, :], energy[:, -1:, :] + end],\n",
        "                                   axis=1)\n",
        "        else:\n",
        "            mask = K.expand_dims(K.cast(mask, K.floatx()))\n",
        "            start_mask = K.cast(K.greater(mask, self.shift_right(mask)), K.floatx())\n",
        "            end_mask = K.cast(K.greater(self.shift_left(mask), mask), K.floatx())\n",
        "            energy = energy + start_mask * start\n",
        "            energy = energy + end_mask * end\n",
        "        return energy\n",
        "\n",
        "    def get_log_normalization_constant(self, input_energy, mask, **kwargs):\n",
        "        \"\"\"Compute logarithm of the normalization constant Z, where\n",
        "        Z = sum exp(-E) -> logZ = log sum exp(-E) =: -nlogZ\n",
        "        \"\"\"\n",
        "        # should have logZ[:, i] == logZ[:, j] for any i, j\n",
        "        logZ = self.recursion(input_energy, mask, return_sequences=False, **kwargs)\n",
        "        return logZ[:, 0]\n",
        "\n",
        "    def get_energy(self, y_true, input_energy, mask):\n",
        "        \"\"\"Energy = a1' y1 + u1' y1 + y1' U y2 + u2' y2 + y2' U y3 + u3' y3 + an' y3\n",
        "        \"\"\"\n",
        "        input_energy = K.sum(input_energy * y_true, 2)  # (B, T)\n",
        "        # (B, T-1)\n",
        "        chain_energy = K.sum(K.dot(y_true[:, :-1, :],\n",
        "                                   self.chain_kernel) * y_true[:, 1:, :], 2)\n",
        "\n",
        "        if mask is not None:\n",
        "            mask = K.cast(mask, K.floatx())\n",
        "            # (B, T-1), mask[:,:-1]*mask[:,1:] makes it work with any padding\n",
        "            chain_mask = mask[:, :-1] * mask[:, 1:]\n",
        "            input_energy = input_energy * mask\n",
        "            chain_energy = chain_energy * chain_mask\n",
        "        total_energy = K.sum(input_energy, -1) + K.sum(chain_energy, -1)  # (B, )\n",
        "\n",
        "        return total_energy\n",
        "\n",
        "    def get_negative_log_likelihood(self, y_true, X, mask):\n",
        "        \"\"\"Compute the loss, i.e., negative log likelihood (normalize by number of time steps)\n",
        "           likelihood = 1/Z * exp(-E) ->  neg_log_like = - log(1/Z * exp(-E)) = logZ + E\n",
        "        \"\"\"\n",
        "        input_energy = self.activation(K.dot(X, self.kernel) + self.bias)\n",
        "        if self.use_boundary:\n",
        "            input_energy = self.add_boundary_energy(input_energy, mask,\n",
        "                                                    self.left_boundary,\n",
        "                                                    self.right_boundary)\n",
        "        energy = self.get_energy(y_true, input_energy, mask)\n",
        "        logZ = self.get_log_normalization_constant(input_energy, mask,\n",
        "                                                   input_length=K.int_shape(X)[1])\n",
        "        nloglik = logZ + energy\n",
        "        if mask is not None:\n",
        "            nloglik = nloglik / K.sum(K.cast(mask, K.floatx()), 1)\n",
        "        else:\n",
        "            nloglik = nloglik / K.cast(K.shape(X)[1], K.floatx())\n",
        "        return nloglik\n",
        "\n",
        "    def step(self, input_energy_t, states, return_logZ=True):\n",
        "        # not in the following  `prev_target_val` has shape = (B, F)\n",
        "        # where B = batch_size, F = output feature dim\n",
        "        # Note: `i` is of float32, due to the behavior of `K.rnn`\n",
        "        prev_target_val, i, chain_energy = states[:3]\n",
        "        t = K.cast(i[0, 0], dtype='int32')\n",
        "        if len(states) > 3:\n",
        "            if K.backend() == 'theano':\n",
        "                m = states[3][:, t:(t + 2)]\n",
        "            else:\n",
        "                m = tf.slice(states[3], [0, t], [-1, 2])\n",
        "            input_energy_t = input_energy_t * K.expand_dims(m[:, 0])\n",
        "            # (1, F, F)*(B, 1, 1) -> (B, F, F)\n",
        "            chain_energy = chain_energy * K.expand_dims(\n",
        "                K.expand_dims(m[:, 0] * m[:, 1]))\n",
        "        if return_logZ:\n",
        "            # shapes: (1, B, F) + (B, F, 1) -> (B, F, F)\n",
        "            energy = chain_energy + K.expand_dims(input_energy_t - prev_target_val, 2)\n",
        "            new_target_val = K.logsumexp(-energy, 1)  # shapes: (B, F)\n",
        "            return new_target_val, [new_target_val, i + 1]\n",
        "        else:\n",
        "            energy = chain_energy + K.expand_dims(input_energy_t + prev_target_val, 2)\n",
        "            min_energy = K.min(energy, 1)\n",
        "            # cast for tf-version `K.rnn\n",
        "            argmin_table = K.cast(K.argmin(energy, 1), K.floatx())\n",
        "            return argmin_table, [min_energy, i + 1]\n",
        "\n",
        "    def recursion(self, input_energy, mask=None, go_backwards=False,\n",
        "                  return_sequences=True, return_logZ=True, input_length=None):\n",
        "        \"\"\"Forward (alpha) or backward (beta) recursion\n",
        "        If `return_logZ = True`, compute the logZ, the normalization constant:\n",
        "        \\[ Z = \\sum_{y1, y2, y3} exp(-E) # energy\n",
        "          = \\sum_{y1, y2, y3} exp(-(u1' y1 + y1' W y2 + u2' y2 + y2' W y3 + u3' y3))\n",
        "          = sum_{y2, y3} (exp(-(u2' y2 + y2' W y3 + u3' y3))\n",
        "          sum_{y1} exp(-(u1' y1' + y1' W y2))) \\]\n",
        "        Denote:\n",
        "            \\[ S(y2) := sum_{y1} exp(-(u1' y1 + y1' W y2)), \\]\n",
        "            \\[ Z = sum_{y2, y3} exp(log S(y2) - (u2' y2 + y2' W y3 + u3' y3)) \\]\n",
        "            \\[ logS(y2) = log S(y2) = log_sum_exp(-(u1' y1' + y1' W y2)) \\]\n",
        "        Note that:\n",
        "              yi's are one-hot vectors\n",
        "              u1, u3: boundary energies have been merged\n",
        "        If `return_logZ = False`, compute the Viterbi's best path lookup table.\n",
        "        \"\"\"\n",
        "        chain_energy = self.chain_kernel\n",
        "        # shape=(1, F, F): F=num of output features. 1st F is for t-1, 2nd F for t\n",
        "        chain_energy = K.expand_dims(chain_energy, 0)\n",
        "        # shape=(B, F), dtype=float32\n",
        "        prev_target_val = K.zeros_like(input_energy[:, 0, :])\n",
        "\n",
        "        if go_backwards:\n",
        "            input_energy = K.reverse(input_energy, 1)\n",
        "            if mask is not None:\n",
        "                mask = K.reverse(mask, 1)\n",
        "\n",
        "        initial_states = [prev_target_val, K.zeros_like(prev_target_val[:, :1])]\n",
        "        constants = [chain_energy]\n",
        "\n",
        "        if mask is not None:\n",
        "            mask2 = K.cast(K.concatenate([mask, K.zeros_like(mask[:, :1])], axis=1),\n",
        "                           K.floatx())\n",
        "            constants.append(mask2)\n",
        "\n",
        "        def _step(input_energy_i, states):\n",
        "            return self.step(input_energy_i, states, return_logZ)\n",
        "\n",
        "        target_val_last, target_val_seq, _ = K.rnn(_step, input_energy,\n",
        "                                                   initial_states,\n",
        "                                                   constants=constants,\n",
        "                                                   input_length=input_length,\n",
        "                                                   unroll=self.unroll)\n",
        "\n",
        "        if return_sequences:\n",
        "            if go_backwards:\n",
        "                target_val_seq = K.reverse(target_val_seq, 1)\n",
        "            return target_val_seq\n",
        "        else:\n",
        "            return target_val_last\n",
        "\n",
        "    def forward_recursion(self, input_energy, **kwargs):\n",
        "        return self.recursion(input_energy, **kwargs)\n",
        "\n",
        "    def backward_recursion(self, input_energy, **kwargs):\n",
        "        return self.recursion(input_energy, go_backwards=True, **kwargs)\n",
        "\n",
        "    def get_marginal_prob(self, X, mask=None):\n",
        "        input_energy = self.activation(K.dot(X, self.kernel) + self.bias)\n",
        "        if self.use_boundary:\n",
        "            input_energy = self.add_boundary_energy(input_energy, mask,\n",
        "                                                    self.left_boundary,\n",
        "                                                    self.right_boundary)\n",
        "        input_length = K.int_shape(X)[1]\n",
        "        alpha = self.forward_recursion(input_energy, mask=mask,\n",
        "                                       input_length=input_length)\n",
        "        beta = self.backward_recursion(input_energy, mask=mask,\n",
        "                                       input_length=input_length)\n",
        "        if mask is not None:\n",
        "            input_energy = input_energy * K.expand_dims(K.cast(mask, K.floatx()))\n",
        "        margin = -(self.shift_right(alpha) + input_energy + self.shift_left(beta))\n",
        "        return self.softmaxNd(margin)\n",
        "\n",
        "    def viterbi_decoding(self, X, mask=None):\n",
        "        input_energy = self.activation(K.dot(X, self.kernel) + self.bias)\n",
        "        if self.use_boundary:\n",
        "            input_energy = self.add_boundary_energy(\n",
        "                input_energy, mask, self.left_boundary, self.right_boundary)\n",
        "\n",
        "        argmin_tables = self.recursion(input_energy, mask, return_logZ=False)\n",
        "        argmin_tables = K.cast(argmin_tables, 'int32')\n",
        "\n",
        "        # backward to find best path, `initial_best_idx` can be any,\n",
        "        # as all elements in the last argmin_table are the same\n",
        "        argmin_tables = K.reverse(argmin_tables, 1)\n",
        "        # matrix instead of vector is required by tf `K.rnn`\n",
        "        initial_best_idx = [K.expand_dims(argmin_tables[:, 0, 0])]\n",
        "        if K.backend() == 'theano':\n",
        "            from theano import tensor as T\n",
        "            initial_best_idx = [T.unbroadcast(initial_best_idx[0], 1)]\n",
        "\n",
        "        def gather_each_row(params, indices):\n",
        "            n = K.shape(indices)[0]\n",
        "            if K.backend() == 'theano':\n",
        "                from theano import tensor as T\n",
        "                return params[T.arange(n), indices]\n",
        "            elif K.backend() == 'tensorflow':\n",
        "                import tensorflow as tf\n",
        "                indices = K.transpose(K.stack([tf.range(n), indices]))\n",
        "                return tf.gather_nd(params, indices)\n",
        "            else:\n",
        "                raise NotImplementedError\n",
        "\n",
        "        def find_path(argmin_table, best_idx):\n",
        "            next_best_idx = gather_each_row(argmin_table, best_idx[0][:, 0])\n",
        "            next_best_idx = K.expand_dims(next_best_idx)\n",
        "            if K.backend() == 'theano':\n",
        "                from theano import tensor as T\n",
        "                next_best_idx = T.unbroadcast(next_best_idx, 1)\n",
        "            return next_best_idx, [next_best_idx]\n",
        "\n",
        "        _, best_paths, _ = K.rnn(find_path, argmin_tables, initial_best_idx,\n",
        "                                 input_length=K.int_shape(X)[1], unroll=self.unroll)\n",
        "        best_paths = K.reverse(best_paths, 1)\n",
        "        best_paths = K.squeeze(best_paths, 2)\n",
        "\n",
        "        return K.one_hot(best_paths, self.units)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9kYauQNVOmy"
      },
      "source": [
        "tag_path=\"/content/gdrive/MyDrive/FYP/200521/index_to_tag200521-2.pickle\"\n",
        "word_path=\"/content/gdrive/MyDrive/FYP/200521/word_to_index200521-2.pickle\"\n",
        "model_path=\"/content/gdrive/MyDrive/FYP/200521/model200521-2.tf\"\n",
        "\n",
        "tagfile=open(tag_path,\"rb\")\n",
        "idx2tag=pickle.load(tagfile)\n",
        "\n",
        "wordfile=open(word_path,\"rb\")\n",
        "word2idx=pickle.load(wordfile)\n",
        "model2=load_model(model_path,custom_objects={'CRF':CRF,'crf_loss':crf_loss,'crf_viterbi_accuracy':crf_viterbi_accuracy})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l6gXExcYVXRO"
      },
      "source": [
        "def preProcess_data(text):\n",
        "    text = text.lower()\n",
        "    new_text = re.sub('[^a-zA-z0-9\\s]','',text)\n",
        "    new_text = re.sub('rt', '', new_text)\n",
        "    return new_text\n",
        "\n",
        "def my_pipeline(text):\n",
        "    X=[word2idx.get(word,1)for word in text]\n",
        "    X=pad_sequences([X],maxlen=74,padding='post')\n",
        "    return X\n",
        "\n",
        "def predict(text):\n",
        "  text=preProcess_data(text)\n",
        "  text_new=text.split()\n",
        "  X=my_pipeline(text_new)\n",
        "  y=model2.predict(X)\n",
        "  Perkataan=[]\n",
        "  Tag=[]\n",
        "  for (w,p) in zip ([text],y):\n",
        "    tags=np.argmax(p,axis=1)\n",
        "    tags=[idx2tag[i]for i in tags if i!=0]\n",
        "    for i in range(len(tags)):\n",
        "     # print(text_new[i],\":\",tags[i])\n",
        "      Perkataan.append(text_new[i])\n",
        "      Tag.append(tags[i])\n",
        "  list_tuple=list(zip(Perkataan,Tag))\n",
        "  return list_tuple\n",
        "   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TdYm_AZhuhJl"
      },
      "source": [
        "from colabcode import ColabCode\n",
        "from fastapi import FastAPI,Form\n",
        "from starlette.responses import HTMLResponse\n",
        "cc=ColabCode(port=13000,code=False)\n",
        "\n",
        "#Initialise model \n",
        "import keras\n",
        "import tensorflow as tf\n",
        "from keras.models import Model, load_model\n",
        "from keras.layers import Input, Embedding, LSTM, Dense, Dropout, TimeDistributed, Bidirectional\n",
        "from keras_contrib.layers import CRF\n",
        "from keras_contrib.losses import  crf_loss\n",
        "from keras_contrib.metrics import crf_viterbi_accuracy\n",
        "import pickle\n",
        "from fastapi import FastAPI\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "from typing import List\n",
        "from pydantic import BaseModel"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pKI5NAkG8k6U",
        "outputId": "e0345d56-7713-495b-a898-01872069e097"
      },
      "source": [
        "p=predict(\"begini deh kita ni nak order air apa pun dok tahu ni air hok tulis panjang panjang itu air apa dia itu gu kelantan dah ada starbucks jangan risau orang kelantan kawan aku lepasan universiti pun tak tahu order starbucks order yang tulis panjang panjang dapat kopi saja\")\n",
        "print(p)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('begini', 'KAD'), ('deh', 'KSR'), ('kita', 'GN1'), ('ni', 'GT-KEP'), ('nak', 'KB-KEP'), ('order', 'FOR'), ('air', 'KN'), ('apa', 'GDT-KTY'), ('pun', 'KPN'), ('dok', 'SL'), ('tahu', 'KA'), ('ni', 'GT-KEP'), ('air', 'KN'), ('hok', 'LD'), ('tulis', 'KK'), ('panjang', 'KA'), ('panjang', 'KA'), ('itu', 'GT'), ('air', 'KN'), ('apa', 'GDT-KTY'), ('dia', 'GN3'), ('itu', 'GT'), ('gu', 'KN'), ('kelantan', 'KN'), ('dah', 'KB-KEP'), ('ada', 'KK'), ('starbucks', 'FOR'), ('jangan', 'KPE'), ('risau', 'KA'), ('orang', 'KN'), ('kelantan', 'KN'), ('kawan', 'KN'), ('aku', 'GN1'), ('lepasan', 'KN'), ('universiti', 'KN'), ('pun', 'KPN'), ('tak', 'KNF-KEP'), ('tahu', 'KA'), ('order', 'FOR'), ('starbucks', 'FOR'), ('order', 'FOR'), ('yang', 'KH'), ('tulis', 'KK'), ('panjang', 'KA'), ('panjang', 'KA'), ('dapat', 'KB'), ('kopi', 'KN'), ('saja', 'KPN')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LVxU6FPY5dJg",
        "outputId": "7c058ae7-b62b-475a-be64-11497a1a5b29"
      },
      "source": [
        "k=predict(\"Lepas ni tengok video youtube Sharnaaz takyah skip ads. Kita sama sama support mamu tolong orang kibar bendera putih #KitaJagaKita\")\n",
        "print(k)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('lepas', 'KH'), ('ni', 'GT-KEP'), ('tengok', 'KK'), ('video', 'KN'), ('youtube', 'KN'), ('sharnaaz', 'KN'), ('takyah', 'KN'), ('skip', 'KN'), ('ads', 'KN'), ('kita', 'GN1'), ('sama', 'KA'), ('sama', 'KA'), ('suppo', 'KN'), ('mamu', 'KN'), ('tolong', 'KPE'), ('orang', 'KN'), ('kibar', 'KN'), ('bendera', 'KN'), ('putih', 'KA'), ('kitajagakita', 'KN')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l8CiD2mNtUTB",
        "outputId": "9cb195c2-f923-49eb-a473-0bd0293f6e87"
      },
      "source": [
        "app=FastAPI(title=\"A NEURAL NETWORK MODEL FOR PGK OF MALAY SOCIAL MEDIA TEXT API\",description=\"with FastAPI and Colab\",version=1.0)\n",
        "\n",
        "#model2=load_model(model_path,custom_objects={'CRF':CRF,'crf_loss':crf_loss,'crf_viterbi_accuracy':crf_viterbi_accuracy})\n",
        "  \n",
        "@app.get('/')\n",
        "def root():\n",
        "  return{\"Hello\":\"Pls add /docs in your path\"}\n",
        "\n",
        "class Item (BaseModel):\n",
        "  content:str\n",
        "\n",
        "\n",
        "@app.post(\"/item\")\n",
        "def post_item(item:Item):\n",
        "  text=item.content\n",
        "  p=predict(text)\n",
        "  return p\n",
        "\n",
        "cc.run_app(app=app)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Public URL: NgrokTunnel: \"https://bb1f949cca05.ngrok.io\" -> \"http://localhost:13000\"\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}